{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.linalg import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dataset_HR import AdSBHDataset\n",
    "from model_HR_new import AdSBHNet\n",
    "from constants import dreal, dcomplex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**This notebook can be run from top to bottom so that for each temperature, you just change the parameters of the very first cell below. You do not need to change anything else, or edit any of the other cells. Run all the cells. The training cell takes about 25mins for 1000 epoch to run.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "**Change these to import different temperatures, and modify how many terms in a, b you want. You don't need to edit anything else in the notebook to run different T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------#\n",
    "# Put here the dataset file, temperature and number of terms for a and b polynomials (N)\n",
    "dataset = AdSBHDataset(file='1607latticeT406.txt')\n",
    "T= 406\n",
    "N = 3\n",
    "#------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Mask the data to use L <1.4 points, and plot (we are now only interested in the real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_L = dataset.L < 1.4\n",
    "L_all     = dataset.L[mask_L]\n",
    "V_all     = dataset.V[mask_L]\n",
    "sigma_all = dataset.sigma[mask_L]\n",
    "\n",
    "print(f\"Dataset: {len(L_all)} points\")\n",
    "print(f\"L range: [{L_all.min():.4f}, {L_all.max():.4f}]\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.errorbar(L_all.numpy(), V_all.real.numpy(),\n",
    "             yerr=sigma_all.real.numpy(), fmt='.', label='Re data')\n",
    "plt.xlabel(r'$T L$'); plt.ylabel(r'$V/T$'); plt.legend(); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Here we have the imaginary part too, but we won't use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.V = dataset.V[dataset.L < 1.4]\n",
    "dataset.sigma = dataset.sigma[dataset.L < 1.4]\n",
    "dataset.L = dataset.L[dataset.L < 1.4]\n",
    "plt.errorbar(dataset.L.numpy(), dataset.V.real.numpy(), yerr=dataset.sigma.real.numpy(), label='Re', fmt='.')\n",
    "plt.errorbar(dataset.L.numpy(), dataset.V.imag.numpy(), yerr=dataset.sigma.imag.numpy(), label='Im', fmt='.')\n",
    "plt.xlabel(f'$T L$')\n",
    "plt.ylabel(f'$V/T$')\n",
    "plt.xlim(0, 1.1*dataset.L.max())\n",
    "plt.ylim(1.1*dataset.V.imag.min().item(), 1.1*dataset.V.real.max().item())\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdSBHNet(N=N, std=0.5)\n",
    "device = L_all.device\n",
    "model.to(device)\n",
    "\n",
    "# start from AdS–BH but enforce a0=-b0<0 and modest magnitude\n",
    "with torch.no_grad():\n",
    "    model.a.zero_()\n",
    "    model.b.zero_()\n",
    "    init_a0 = -0.3\n",
    "    model.a[0] = init_a0\n",
    "    model.b[0] = -init_a0\n",
    "    model.logcoef.fill_(0.0)   \n",
    "    model.shift.fill_(0.0)    \n",
    "\n",
    "print(\"Initial parameters:\")\n",
    "print(\"  a =\", model.a.detach().cpu().numpy())\n",
    "print(\"  b =\", model.b.detach().cpu().numpy())\n",
    "print(f\"  coef = {model.logcoef.exp().item():.3f}, shift = {model.shift.item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_branch(model, zmin=0.02, zmax=0.9995, Nc=2500):\n",
    "    dev, dt = model.a.device, model.a.dtype\n",
    "    zs = torch.linspace(zmin, zmax, Nc, dtype=dt, device=dev)\n",
    "    Lc = model.integrate_L(zs).real\n",
    "    Vc = model.logcoef.exp() * model.integrate_V(zs).real\n",
    "    idx = torch.argsort(Lc)\n",
    "    Lc, Vc = Lc[idx], Vc[idx]\n",
    "    imax = torch.argmax(Lc)\n",
    "    return Lc[:imax+1], Vc[:imax+1]\n",
    "\n",
    "\n",
    "def V_of_L_interp_real(model, L_query,\n",
    "                       zmin=0.02, zmax=0.9997, zs_num=4000, q=2.7, eps=1e-12):\n",
    "    dev, dt = model.a.device, model.a.dtype\n",
    "    Lq = torch.as_tensor(L_query, dtype=dt, device=dev).reshape(-1)\n",
    "\n",
    "    u  = torch.linspace(0.0, 1.0, zs_num, dtype=dt, device=dev)\n",
    "    zs = zmin + (zmax - zmin) * (u**q)\n",
    "\n",
    "    Lc = model.integrate_L(zs).real\n",
    "    Vc = model.logcoef.exp() * model.integrate_V(zs).real\n",
    "\n",
    "    idx = torch.argsort(Lc)\n",
    "    Lc, Vc = Lc[idx], Vc[idx]\n",
    "\n",
    "    dL = torch.diff(Lc)\n",
    "    keep = torch.cat([torch.tensor([True], device=dev), dL > 0])\n",
    "    Lm, Vm = Lc[keep], Vc[keep]\n",
    "\n",
    "    if Lm.numel() < 2:\n",
    "        Lm, Vm = Lc, Vc\n",
    "\n",
    "    pos = torch.searchsorted(Lm, Lq, right=True)\n",
    "    i0 = (pos-1).clamp(0, Lm.numel()-2)\n",
    "    i1 = i0+1\n",
    "    x0, x1 = Lm[i0], Lm[i1]\n",
    "    y0, y1 = Vm[i0], Vm[i1]\n",
    "\n",
    "    w     = (Lq - x0) / (x1 - x0 + eps)\n",
    "    v_lin = y0 + w*(y1-y0)\n",
    "\n",
    "    mL = (Vm[1]-Vm[0])   / (Lm[1]-Lm[0]   + eps)\n",
    "    mR = (Vm[-1]-Vm[-2]) / (Lm[-1]-Lm[-2] + eps)\n",
    "\n",
    "    v_left  = Vm[0]  + mL*(Lq - Lm[0])\n",
    "    v_right = Vm[-1] + mR*(Lq - Lm[-1])\n",
    "\n",
    "    v = torch.where(Lq <  Lm[0],  v_left,  v_lin)\n",
    "    v = torch.where(Lq >  Lm[-1], v_right, v)\n",
    "\n",
    "    return v, (Lm[0].detach(), Lm[-1].detach())\n",
    "\n",
    "\n",
    "def smoothness_C2(val_fn, z):\n",
    "    v = val_fn(z)\n",
    "    d2 = v[:-2] - 2*v[1:-1] + v[2:]\n",
    "    return (d2.real**2).mean()\n",
    "\n",
    "\n",
    "def uv_horizon_penalty(model):\n",
    "    z0 = torch.tensor(1e-3,  dtype=model.a.dtype, device=model.a.device)\n",
    "    zh = torch.tensor(0.995, dtype=model.a.dtype, device=model.a.device)\n",
    "\n",
    "    f0   = model.eval_f(z0)\n",
    "    g0   = model.eval_g(z0)\n",
    "    fhor = model.eval_f(zh)\n",
    "\n",
    "    def _r2(t, target):\n",
    "        if torch.is_complex(t):\n",
    "            return (t.real - target)**2 + t.imag**2\n",
    "        else:\n",
    "            return (t - target)**2\n",
    "\n",
    "    return _r2(f0, 1.0) + _r2(g0, 1.0) + _r2(fhor, 0.0)\n",
    "\n",
    "def swallowtail_reg(current_model, z_grid, L0_ref, V0_ref,\n",
    "                    alpha_L=1.0, alpha_V=0.3):\n",
    "    \"\"\"\n",
    "    Penalize deviations of the real parametric curve (L(z*), V(z*)) from\n",
    "    the reference AdS–BH swallowtail.\n",
    "\n",
    "    alpha_L: relative weight on matching L(z*)\n",
    "    alpha_V: relative weight on matching V(z*)\n",
    "    \"\"\"\n",
    "    Lc = current_model.integrate_L(z_grid).real\n",
    "    Vc = current_model.logcoef.exp() * current_model.integrate_V(z_grid).real\n",
    "\n",
    "    loss_L = ((Lc - L0_ref)**2).mean()\n",
    "    loss_V = ((Vc - V0_ref)**2).mean()\n",
    "\n",
    "    return alpha_L * loss_L + alpha_V * loss_V\n",
    "\n",
    "\n",
    "def nec_penalty(model, num_samples=100, weight=100.0):\n",
    "    \"\"\"\n",
    "    Compute NEC (Null Energy Condition) violation penalty.\n",
    "    \n",
    "    NEC requires: -(3/2z) * [a'(z) + b'(z)] >= 0 for all z in [0, 1]\n",
    "    This means: (a0+b0) + 2(a1+b1)z + 3(a2+b2)z^2 + 4(a3+b3)z^3 <= 0\n",
    "    \n",
    "    We penalize violations where P(z) > 0.\n",
    "    \n",
    "    Args:\n",
    "        model: The AdSBHNet model\n",
    "        num_samples: Number of z points to check\n",
    "        weight: Multiplier for the penalty (default 100.0)\n",
    "    \n",
    "    Returns:\n",
    "        Weighted penalty value (tensor)\n",
    "    \"\"\"\n",
    "    a = model.a\n",
    "    b = model.b\n",
    "    device = a.device\n",
    "    dtype = a.dtype\n",
    "    \n",
    "    z = torch.linspace(0.01, 0.99, num_samples, dtype=dtype, device=device)\n",
    "    \n",
    "    # Compute polynomial P(z) = sum (n+1)(an+bn)z^n\n",
    "    P = (a[0] + b[0]) + 2*(a[1] + b[1])*z + 3*(a[2] + b[2])*z**2 + 4*(a[3] + b[3])*z**3\n",
    "    \n",
    "    # NEC violation if P > 0\n",
    "    violation = torch.relu(P).max()\n",
    "    \n",
    "    return weight * violation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    Lm, Vm = connected_branch(model)\n",
    "    Ld = L_all.to(device=Lm.device, dtype=Lm.dtype)\n",
    "    Vd = V_all.real.to(device=Lm.device, dtype=Lm.dtype)\n",
    "\n",
    "    mask = (Ld >= Lm[0]) & (Ld <= Lm[-1])\n",
    "    L_fit = Ld[mask]\n",
    "    Y_fit = Vd[mask]\n",
    "\n",
    "    # model V at those L (via interpolation)\n",
    "    def _interp_1d(x, y, xq, eps=1e-12):\n",
    "        dt, dev = x.dtype, x.device\n",
    "        xq = torch.as_tensor(xq, dtype=dt, device=dev).reshape(-1)\n",
    "        pos = torch.searchsorted(x, xq, right=True)\n",
    "        i0 = (pos-1).clamp(0, x.numel()-2); i1 = i0+1\n",
    "        x0, x1 = x[i0], x[i1]; y0, y1 = y[i0], y[i1]\n",
    "        w  = (xq - x0) / (x1 - x0 + eps)\n",
    "        v_lin = y0 + w*(y1-y0)\n",
    "        mL = (y[1]-y[0])/(x[1]-x[0]+eps)\n",
    "        mR = (y[-1]-y[-2])/(x[-1]-x[-2]+eps)\n",
    "        v_left  = y[0]  + mL*(xq - x[0])\n",
    "        v_right = y[-1] + mR*(xq - x[-1])\n",
    "        v = torch.where(xq <  x[0],  v_left,  v_lin)\n",
    "        v = torch.where(xq >  x[-1], v_right, v)\n",
    "        return v\n",
    "\n",
    "    X_fit = _interp_1d(Lm, Vm, L_fit) / model.logcoef.exp()\n",
    "\n",
    "    fit = LinearRegression().fit(X_fit.cpu().numpy().reshape(-1,1),\n",
    "                                 Y_fit.cpu().numpy())\n",
    "    slope = max(float(fit.coef_[0]), 1e-12)\n",
    "    shift = float(fit.intercept_)\n",
    "\n",
    "    model.logcoef = nn.Parameter(torch.tensor(np.log(slope),\n",
    "                                              dtype=model.a.dtype,\n",
    "                                              device=model.a.device))\n",
    "    model.shift   = nn.Parameter(torch.tensor(shift,\n",
    "                                              dtype=model.a.dtype,\n",
    "                                              device=model.a.device))\n",
    "\n",
    "print(f\"LS init: coef = {model.logcoef.exp().item():.3f}, shift = {model.shift.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "dev, dt = model.a.device, model.a.dtype\n",
    "def compute_Lmax(model, zmin=0.02, zmax=0.995, Nc=2000):\n",
    "    \"\"\"Compute the maximum L value on the real branch.\"\"\"\n",
    "    dev, dt = model.a.device, model.a.dtype\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            zs = torch.linspace(zmin, zmax, Nc, dtype=dt, device=dev)\n",
    "            Lc = model.integrate_L(zs).real\n",
    "            if not torch.isfinite(Lc).all():\n",
    "                return torch.tensor(0.4, device=dev, dtype=dt)  # Return reasonable default\n",
    "            Lmax = Lc.max()\n",
    "            if Lmax < 0.1:  # Too small\n",
    "                return torch.tensor(0.4, device=dev, dtype=dt)\n",
    "            return Lmax\n",
    "        except:\n",
    "            return torch.tensor(0.4, device=dev, dtype=dt)\n",
    "\n",
    "\n",
    "def nec_penalty_simple(model, num_samples=100):\n",
    "    \"\"\"\n",
    "    NEC penalty: penalize where d(a+b)/dz > 0\n",
    "    For polynomials: a(z) = a0*z + a1*z^2 + ..., b(z) = b0*z + b1*z^2 + ...\n",
    "    d(a+b)/dz = (a0+b0) + 2(a1+b1)z + 3(a2+b2)z^2 + ...\n",
    "    \"\"\"\n",
    "    z = torch.linspace(1e-3, 0.98, num_samples, dtype=model.a.dtype, device=model.a.device)\n",
    "\n",
    "    # Compute derivative\n",
    "    N = model.a.numel()\n",
    "    deriv = torch.zeros_like(z)\n",
    "    for i in range(N):\n",
    "        deriv = deriv + (i + 1) * (model.a[i] + model.b[i]) * (z ** i)\n",
    "\n",
    "    # NEC violation when derivative > 0\n",
    "    violation = torch.relu(deriv).mean()\n",
    "    return violation\n",
    "\n",
    "\n",
    "def monotonicity_penalty(model, num_samples=100):\n",
    "    \"\"\"\n",
    "    Soft penalty for monotonicity: f(z)/z^4 should be decreasing.\n",
    "    \n",
    "    For f(z) = (1-z^4)·exp(a(z)), monotonicity requires:\n",
    "        z·a'(z)·(1-z^4) < 4\n",
    "    \n",
    "    This is a SOFT constraint - we only penalize when severely violated.\n",
    "    \"\"\"\n",
    "    device = model.a.device\n",
    "    dtype = model.a.dtype\n",
    "    \n",
    "    z = torch.linspace(0.05, 0.95, num_samples, dtype=dtype, device=device)\n",
    "    \n",
    "    # Compute a'(z)\n",
    "    a_prime = model.eval_da(z)\n",
    "    \n",
    "    # Monotonicity condition: z·a'(z)·(1-z^4) < 4\n",
    "    lhs = z * a_prime * (1 - z**4)\n",
    "    \n",
    "    # Only penalize SEVERE violations (lhs > 5.0)\n",
    "    # This gives the model room to explore while preventing crashes\n",
    "    violation = torch.relu(lhs - 5.0)\n",
    "    \n",
    "    return violation.mean()\n",
    "\n",
    "\n",
    "def compute_loss_physics(model, Lb, Vb, sb,\n",
    "                        target_Lmax=0.5,\n",
    "                        lam_data=1.0,\n",
    "                        lam_Lmax=200.0,\n",
    "                        lam_nec=10.0,\n",
    "                        lam_a0b0=10.0,\n",
    "                        lam_magnitude=1.0,\n",
    "                        lam_monotonicity=10.0,\n",
    "                        lam_reg=1e-3):\n",
    "    \"\"\"\n",
    "    Physics-informed loss (works for arbitrary N):\n",
    "      1. Fit data points on the STABLE branch (L < L_max)\n",
    "      2. Penalize L_max deviation from target range [0.35, 0.7]\n",
    "      3. Enforce NEC constraint\n",
    "      4. Soft constraint: a0 + b0 ≈ 0\n",
    "      5. Encourage non-trivial parameters (especially higher-order)\n",
    "      6. Soft monotonicity penalty (gentle)\n",
    "      7. Light L2 regularization to prevent explosion\n",
    "    \"\"\"\n",
    "    dev, dt = model.a.device, model.a.dtype\n",
    "    N = model.a.numel()  # Get actual number of parameters\n",
    "\n",
    "    Lb = Lb.to(dev, dtype=dt)\n",
    "    Vb = Vb.to(dev)\n",
    "    sb = sb.to(dev)\n",
    "\n",
    "    # Conservative clamping to prevent explosion (wider range)\n",
    "    with torch.no_grad():\n",
    "        model.a.clamp_(-2.5, 2.5)\n",
    "        model.b.clamp_(-2.5, 2.5)\n",
    "        model.logcoef.clamp_(-3.0, 1.0)\n",
    "        model.shift.clamp_(-5.0, 15.0)\n",
    "\n",
    "    try:\n",
    "        # Compute L_max\n",
    "        Lmax_val = compute_Lmax(model)\n",
    "\n",
    "        # Penalty for L_max outside target range [0.35, 0.7]\n",
    "        Lmax_penalty = torch.tensor(0.0, device=dev, dtype=dt)\n",
    "        if Lmax_val < 0.35:\n",
    "            Lmax_penalty = lam_Lmax * (0.35 - Lmax_val)**2\n",
    "        elif Lmax_val > 0.7:\n",
    "            Lmax_penalty = lam_Lmax * (Lmax_val - 0.7)**2\n",
    "\n",
    "        # Soft constraint: a0 + b0 ≈ 0 (instead of hard constraint)\n",
    "        a0b0_penalty = lam_a0b0 * (model.a[0] + model.b[0])**2\n",
    "\n",
    "        # Encourage larger magnitude for highest-order terms (dynamically adapt to N)\n",
    "        magnitude_penalty = torch.tensor(0.0, device=dev, dtype=dt)\n",
    "        if N > 0:\n",
    "            # Penalize if highest-order terms are too small\n",
    "            last_idx = N - 1\n",
    "            magnitude_penalty = lam_magnitude * (\n",
    "                torch.relu(0.5 - model.a[last_idx].abs()) +  # penalize if |a[N-1]| < 0.5\n",
    "                torch.relu(0.5 - model.b[last_idx].abs())     # penalize if |b[N-1]| < 0.5\n",
    "            )\n",
    "\n",
    "        # Get V predictions\n",
    "        V_core, (Lmin, Lmax_interp) = V_of_L_interp_real(model, Lb, zs_num=2000)\n",
    "\n",
    "        if not torch.isfinite(V_core).all():\n",
    "            penalty = 1e4 + lam_reg * ((model.a**2).sum() + (model.b**2).sum())\n",
    "            return penalty, {'error': 'V_core_nan', 'Lmax': float(Lmax_val.item())}\n",
    "\n",
    "        # Only fit points on the STABLE branch: L < L_max\n",
    "        # (Stable = monotonically increasing L with z*)\n",
    "        mask = (Lb < Lmax_val) & (Lb >= Lmin)\n",
    "\n",
    "        if mask.sum() < 2:\n",
    "            # If insufficient points, use ALL points and add large Lmax penalty\n",
    "            mask = torch.ones_like(Lb, dtype=torch.bool)\n",
    "            Lmax_penalty = Lmax_penalty + 5000.0 * (0.5 - Lmax_val)**2  # Force larger Lmax\n",
    "            if mask.sum() < 2:\n",
    "                penalty = 1e3 + Lmax_penalty\n",
    "                return penalty, {'error': 'no_data', 'Lmax': float(Lmax_val.item()), 'mask_count': 0}\n",
    "\n",
    "        V_pred = V_core[mask] + model.shift\n",
    "        V_true = Vb.real[mask]\n",
    "\n",
    "        # Weighted MSE by error bars\n",
    "        sigma = sb.real[mask]\n",
    "        weights = 1.0 / (sigma**2 + eps)\n",
    "        weights = weights / weights.sum()  # normalize\n",
    "\n",
    "        data_loss = lam_data * (weights * (V_pred - V_true)**2).sum()\n",
    "\n",
    "        # NEC penalty\n",
    "        nec_loss = lam_nec * nec_penalty_simple(model)\n",
    "\n",
    "        # Monotonicity penalty (gentle - only penalizes severe violations)\n",
    "        mono_loss = lam_monotonicity * monotonicity_penalty(model, num_samples=50)\n",
    "\n",
    "        # L2 regularization (light touch)\n",
    "        reg = lam_reg * ((model.a**2).sum() + (model.b**2).sum())\n",
    "\n",
    "        loss = data_loss + Lmax_penalty + nec_loss + a0b0_penalty + magnitude_penalty + mono_loss + reg\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            penalty = 1e4 + Lmax_penalty + a0b0_penalty + reg\n",
    "            return penalty, {'error': 'loss_nan', 'Lmax': float(Lmax_val.item())}\n",
    "\n",
    "        return loss, {\n",
    "            'data_loss': float(data_loss.item()),\n",
    "            'Lmax': float(Lmax_val.item()),\n",
    "            'Lmax_penalty': float(Lmax_penalty.item()),\n",
    "            'nec_loss': float(nec_loss.item()),\n",
    "            'a0b0_penalty': float(a0b0_penalty.item()),\n",
    "            'mag_penalty': float(magnitude_penalty.item()),\n",
    "            'mono_loss': float(mono_loss.item()),\n",
    "            'mask_count': int(mask.sum().item())\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        penalty = 1e4 + lam_reg * ((model.a**2).sum() + (model.b**2).sum())\n",
    "        return penalty, {'error': str(e)[:50], 'Lmax': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**The training will take 22 minutes for N=3 and 25 minutes for N=4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.a.device\n",
    "dt     = model.a.dtype\n",
    "eps    = 1e-12\n",
    "\n",
    "model = AdSBHNet(N=N, std=0.1)\n",
    "model.to(device)\n",
    "\n",
    "# full dataset\n",
    "L_all     = dataset.L.to(device=device, dtype=dt)\n",
    "V_all     = dataset.V.to(device=device, dtype=torch.cdouble)\n",
    "sigma_all = dataset.sigma.to(device=device, dtype=torch.cdouble)\n",
    "\n",
    "# train/val split\n",
    "N_tot = L_all.shape[0]\n",
    "perm  = torch.randperm(N_tot, device=device)\n",
    "n_val = max(4, int(0.2 * N_tot))\n",
    "idx_val   = perm[:n_val]\n",
    "idx_train = perm[n_val:]\n",
    "\n",
    "L_train, V_train, sigma_train = L_all[idx_train], V_all[idx_train], sigma_all[idx_train]\n",
    "L_val,   V_val,   sigma_val   = L_all[idx_val],   V_all[idx_val],   sigma_all[idx_val]\n",
    "\n",
    "print(f\"Training on {len(L_train)} points, validating on {len(L_val)} points\")\n",
    "print(\"This training will take about 25 minutes for 1000 epochs\")\n",
    "print(f\"L_train range: [{L_train.min():.3f}, {L_train.max():.3f}]\")\n",
    "print(f\"Model has N={N} parameters per polynomial\")\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "# Start with parameters that give larger L_max\n",
    "with torch.no_grad():\n",
    "    model.a.zero_()\n",
    "    model.b.zero_()\n",
    "\n",
    "    # \n",
    "    model.a[0] = -0.08\n",
    "    model.b[0] = -0.025\n",
    "\n",
    "    # Higher-order terms \n",
    "    if N > 1:\n",
    "        model.a[1:] = torch.abs(torch.randn(N-1, dtype=dt, device=device)) * 0.1 + 0.05\n",
    "        model.b[1:] = torch.abs(torch.randn(N-1, dtype=dt, device=device)) * 0.1 + 0.05\n",
    "\n",
    "    # Initialize logcoef and shift\n",
    "    model.logcoef.fill_(np.log(0.20))\n",
    "    model.shift.fill_(5.5)\n",
    "\n",
    "print(f\"Initial parameters:\")\n",
    "print(f\"  a = {model.a.detach().cpu().numpy()}\")\n",
    "print(f\"  b = {model.b.detach().cpu().numpy()}\")\n",
    "print(f\"  Check: a[0]={model.a[0].item():.4f} < b[0]={model.b[0].item():.4f} < 0? {model.a[0].item() < model.b[0].item() < 0}\")\n",
    "print(f\"  coef = {model.logcoef.exp().item():.3f}, shift = {model.shift.item():.3f}\")\n",
    "\n",
    "# Check initial Lmax\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        init_Lmax = compute_Lmax(model)\n",
    "        print(f\"  Initial L_max = {init_Lmax:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not compute initial L_max: {e}\")\n",
    "\n",
    "train_ds     = torch.utils.data.TensorDataset(L_train, V_train, sigma_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=len(L_train), shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam([\n",
    "    {'params': [model.a],       'lr': 5e-3},\n",
    "    {'params': [model.b],       'lr': 5e-3},\n",
    "    {'params': [model.shift],   'lr': 5e-3},\n",
    "    {'params': [model.logcoef], 'lr': 3e-3},\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.7, patience=50)\n",
    "\n",
    "EPOCHS = 1000\n",
    "a_hist, b_hist = [], []\n",
    "train_loss_hist, val_loss_hist = [], []\n",
    "Lmax_hist = []\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_state = None\n",
    "prev_lr = opt.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "print(\"  Early epochs: PUSH parameters away from zero\")\n",
    "print(\"  Later epochs: Allow optimization to fine-tune\\n\")\n",
    "\n",
    "nan_count = 0\n",
    "max_nan_tolerance = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running = 0.0; n_batches = 0\n",
    "    epoch_info = {}\n",
    "\n",
    "    # bounds, staged\n",
    "    if epoch <= 200:\n",
    "        a0_min, a0_max = -0.2, -0.005\n",
    "        b0_min, b0_max = -0.2, -0.002\n",
    "        a0_target_mag = 0.1\n",
    "        b0_target_mag = 0.05\n",
    "    elif epoch <= 400:\n",
    "        a0_min, a0_max = -0.4, -0.003\n",
    "        b0_min, b0_max = -0.35, -0.001\n",
    "        a0_target_mag = 0.15\n",
    "        b0_target_mag = 0.08\n",
    "    elif epoch <= 700:\n",
    "        a0_min, a0_max = -0.6, -0.002\n",
    "        b0_min, b0_max = -0.5, -0.001\n",
    "        a0_target_mag = 0.2\n",
    "        b0_target_mag = 0.1\n",
    "    else:\n",
    "        a0_min, a0_max = -0.8, -0.001\n",
    "        b0_min, b0_max = -0.7, -0.0005\n",
    "        a0_target_mag = 0.25\n",
    "        b0_target_mag = 0.12\n",
    "\n",
    "    # MAGNITUDE PENALTY schedule\n",
    "    if epoch < 300:\n",
    "        lam_mag_encourage = 20.0\n",
    "    elif epoch < 600:\n",
    "        lam_mag_encourage = 8.0\n",
    "    else:\n",
    "        lam_mag_encourage = 2.0\n",
    "\n",
    "    # Standard magnitude penalty\n",
    "    if epoch < 200:\n",
    "        lam_magnitude_current = 3.0\n",
    "    elif epoch < 400:\n",
    "        lam_magnitude_current = 1.5\n",
    "    else:\n",
    "        lam_magnitude_current = 0.5\n",
    "\n",
    "    # Lmax penalty\n",
    "    if epoch < 300:\n",
    "        lam_Lmax_current = 100.0\n",
    "    elif epoch < 600:\n",
    "        lam_Lmax_current = 200.0\n",
    "    else:\n",
    "        lam_Lmax_current = 300.0\n",
    "\n",
    "    # Monotonicity penalty schedule, starts at 0\n",
    "    if epoch < 100:\n",
    "        lam_monotonicity_current = 0.0   # No penalty early on\n",
    "    elif epoch < 300:\n",
    "        lam_monotonicity_current = 5.0   # Very gentle\n",
    "    elif epoch < 600:\n",
    "        lam_monotonicity_current = 10.0  # Mild\n",
    "    else:\n",
    "        lam_monotonicity_current = 15.0  # Moderate\n",
    "\n",
    "    for Lb, Vb, sb in train_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        try:\n",
    "            loss, info = compute_loss_physics(model, Lb, Vb, sb,\n",
    "                                              target_Lmax=0.5,\n",
    "                                              lam_data=10.0,\n",
    "                                              lam_Lmax=lam_Lmax_current,\n",
    "                                              lam_nec=0.0,\n",
    "                                              lam_a0b0=0.0,\n",
    "                                              lam_magnitude=lam_magnitude_current,\n",
    "                                              lam_monotonicity=lam_monotonicity_current,\n",
    "                                              lam_reg=1e-3)\n",
    "\n",
    "            if 'error' in info:\n",
    "                print(f\"Epoch {epoch}: {info['error']}, Lmax={info.get('Lmax', 0):.3f}\")\n",
    "                break\n",
    "\n",
    "            # MAGNITUDE ENCOURAGEMENT (works for any N)\n",
    "            mag_penalty = torch.tensor(0.0, device=device, dtype=dt)\n",
    "\n",
    "            # Push a[0], b[0] to have magnitude >= target\n",
    "            if model.a[0].abs() < a0_target_mag:\n",
    "                mag_penalty += lam_mag_encourage * (a0_target_mag - model.a[0].abs())**2\n",
    "            if model.b[0].abs() < b0_target_mag:\n",
    "                mag_penalty += lam_mag_encourage * (b0_target_mag - model.b[0].abs())**2\n",
    "\n",
    "            # Encourage higher-order terms (dynamically adapts to N)\n",
    "            if N > 1:\n",
    "                for i in range(1, N):\n",
    "                    if model.a[i] < 0.05:\n",
    "                        mag_penalty += 0.5 * lam_mag_encourage * (0.05 - model.a[i])**2\n",
    "                    if model.b[i] < 0.05:\n",
    "                        mag_penalty += 0.5 * lam_mag_encourage * (0.05 - model.b[i])**2\n",
    "\n",
    "            loss = loss + mag_penalty\n",
    "\n",
    "            # Check for NaN\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Epoch {epoch}: NaN/Inf loss detected!\")\n",
    "                nan_count += 1\n",
    "                if nan_count > max_nan_tolerance:\n",
    "                    print(\"Too many NaNs - stopping\")\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Check for nan gradients\n",
    "            has_nan_grad = False\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    print(f\"Epoch {epoch}: NaN gradient in {name}\")\n",
    "                    has_nan_grad = True\n",
    "                    break\n",
    "\n",
    "            if has_nan_grad:\n",
    "                nan_count += 1\n",
    "                continue\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Epoch {epoch}: Exception: {e}\")\n",
    "            nan_count += 1\n",
    "            if nan_count > max_nan_tolerance:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # constraints of a and b\n",
    "        with torch.no_grad():\n",
    "            # a[0] < b[0] < 0\n",
    "            model.a[0].clamp_(a0_min, a0_max)\n",
    "            model.b[0].clamp_(b0_min, b0_max)\n",
    "\n",
    "            if model.a[0] >= model.b[0]:\n",
    "                avg = (model.a[0] + model.b[0]) / 2.0\n",
    "                model.a[0] = avg - 0.01\n",
    "                model.b[0] = avg + 0.01\n",
    "                model.a[0].clamp_(a0_min, a0_max)\n",
    "                model.b[0].clamp_(b0_min, b0_max)\n",
    "\n",
    "            # Higher-order terms > 0\n",
    "            if N > 1:\n",
    "                model.a[1:].clamp_(min=1e-5)\n",
    "                model.b[1:].clamp_(min=1e-5)\n",
    "\n",
    "            # Prevent explosion\n",
    "            model.a.clamp_(-2.0, 2.0)\n",
    "            model.b.clamp_(-2.0, 2.0)\n",
    "\n",
    "        running += float(loss.item())\n",
    "        n_batches += 1\n",
    "        epoch_info = info\n",
    "        epoch_info['mag_encourage'] = float(mag_penalty.item())\n",
    "\n",
    "    if n_batches == 0 or nan_count > max_nan_tolerance:\n",
    "        print(f\"Training stopped at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "    train_loss = running / max(n_batches, 1)\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    # Check if learning rate was reduced\n",
    "    current_lr = opt.param_groups[0]['lr']\n",
    "    if current_lr < prev_lr:\n",
    "        print(f\"Epoch {epoch}: Reduced learning rate to {current_lr:.2e}\")\n",
    "        prev_lr = current_lr\n",
    "\n",
    "    # Track best model\n",
    "    if train_loss < best_loss and not torch.isnan(torch.tensor(train_loss)):\n",
    "        best_loss = train_loss\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            val_loss, val_info = compute_loss_physics(model, L_val, V_val, sigma_val,\n",
    "                                                      target_Lmax=0.5, lam_data=10.0,\n",
    "                                                      lam_Lmax=lam_Lmax_current, lam_nec=0.0,\n",
    "                                                      lam_a0b0=0.0, lam_magnitude=lam_magnitude_current,\n",
    "                                                      lam_monotonicity=lam_monotonicity_current,\n",
    "                                                      lam_reg=1e-3)\n",
    "            loss_val = float(val_loss.item()) if torch.isfinite(val_loss) else float('nan')\n",
    "        except:\n",
    "            loss_val = float('nan')\n",
    "            val_info = {}\n",
    "\n",
    "    a_hist.append(model.a.detach().cpu().numpy().copy())\n",
    "    b_hist.append(model.b.detach().cpu().numpy().copy())\n",
    "    train_loss_hist.append(train_loss)\n",
    "    val_loss_hist.append(loss_val)\n",
    "    Lmax_hist.append(epoch_info.get('Lmax', 0.0))\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == 1 or epoch in [200, 400, 700]:\n",
    "        mono_val = epoch_info.get('mono_loss', 0)\n",
    "        print(f\"epoch {epoch:4d} | loss={train_loss:.2e} | Lmax={epoch_info.get('Lmax', 0):.3f} | \" +\n",
    "              f\"Lmax_pen={epoch_info.get('Lmax_penalty', 0):.1e} | mono={mono_val:.2e}\")\n",
    "        print(f\"          | data={epoch_info.get('data_loss', 0):.2e} | \" +\n",
    "              f\"mag_enc={epoch_info.get('mag_encourage', 0):.2e} (λ={lam_mag_encourage:.1f})\")\n",
    "        print(f\"          | a={model.a.detach().cpu().numpy()}\")\n",
    "        print(f\"          | b={model.b.detach().cpu().numpy()}\")\n",
    "        print(f\"          | |a[0]|={model.a[0].abs().item():.3f} (target>{a0_target_mag:.2f}), \" +\n",
    "              f\"|b[0]|={model.b[0].abs().item():.3f} (target>{b0_target_mag:.2f})\")\n",
    "        print(f\"          | coef={model.logcoef.exp().item():.4f}, shift={model.shift.item():.4f}\")\n",
    "\n",
    "# Restore best model\n",
    "if best_state is not None:\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    print(f\"\\nRestored best model with loss={best_loss:.3e}\")\n",
    "\n",
    "print(\"\\nFinal parameters:\")\n",
    "print(f\"a = {model.a.detach().cpu().numpy()}\")\n",
    "print(f\"b = {model.b.detach().cpu().numpy()}\")\n",
    "print(f\"Magnitudes:\")\n",
    "print(f\"  |a[0]| = {model.a[0].abs().item():.4f}\")\n",
    "print(f\"  |b[0]| = {model.b[0].abs().item():.4f}\")\n",
    "if N > 1:\n",
    "    print(f\"  |a[N-1]| = {model.a[N-1].abs().item():.4f}\")\n",
    "    print(f\"  |b[N-1]| = {model.b[N-1].abs().item():.4f}\")\n",
    "print(f\"Constraint verification:\")\n",
    "print(f\"  a[0] = {model.a[0].item():.6f}, b[0] = {model.b[0].item():.6f}\")\n",
    "print(f\"  a[0] < b[0] < 0? {model.a[0].item() < model.b[0].item() < 0}\")\n",
    "if N > 1:\n",
    "    print(f\"  a[1:] > 0? {(model.a[1:] > 0).all().item()}\")\n",
    "    print(f\"  b[1:] > 0? {(model.b[1:] > 0).all().item()}\")\n",
    "print(f\"coef = {model.logcoef.exp().item():.4f}, shift = {model.shift.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        final_Lmax = compute_Lmax(model)\n",
    "        print(f\"Final L_max = {final_Lmax:.4f}\")\n",
    "    except:\n",
    "        print(\"Final L_max could not be computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    zs = torch.linspace(0.05, 0.99, 800, dtype=dt, device=device)\n",
    "    Lc = model.integrate_L(zs).real\n",
    "    Vc = model.logcoef.exp() * model.integrate_V(zs).real + model.shift\n",
    "    \n",
    "    # Only plot the STABLE branch (before L_max where dL/dz* > 0)\n",
    "    # Find where L starts decreasing\n",
    "    dL = torch.diff(Lc)\n",
    "    # Find the index where L reaches maximum (start of unstable branch)\n",
    "    imax = torch.argmax(Lc)\n",
    "    \n",
    "    # Only keep the stable branch (monotonically increasing L)\n",
    "    Lc_stable = Lc[:imax+1]\n",
    "    Lc_unstable = Lc[imax+1:]\n",
    "    Vc_stable = Vc[:imax+1]\n",
    "    Vc_unstable = Vc[imax+1:]\n",
    "    \n",
    "    print(f\"L_max = {Lc[imax].item():.4f} at z* = {zs[imax].item():.4f}\")\n",
    "    print(f\"Stable branch: {len(Lc_stable)} points, L range: [{Lc_stable.min():.4f}, {Lc_stable.max():.4f}]\")\n",
    "    print(f\"V range on stable branch: [{Vc_stable.min():.4f}, {Vc_stable.max():.4f}]\")\n",
    "\n",
    "plt.figure(figsize=(6.5,4.5))\n",
    "plt.plot(Lc_stable.cpu(), Vc_stable.cpu(), '-',color='navy', linewidth=2, label=\"model\")\n",
    "plt.plot(Lc_unstable.cpu(), Vc_unstable.cpu(), '-', color='navy', linewidth=2)\n",
    "plt.errorbar(dataset.L.cpu(), dataset.V.real.cpu(),\n",
    "             yerr=dataset.sigma.real.cpu(), fmt='o', color='gray', \n",
    "             markersize=6, capsize=3, label='data', alpha=0.8)\n",
    "#plt.axvline(Lc[imax].cpu(), color='green', linestyle='--', alpha=0.5, label=f'L_max={Lc[imax].item():.3f}')\n",
    "plt.ylim(-4, None)\n",
    "plt.xlim(0, 1.05*dataset.L.max().item())\n",
    "plt.title(f\"T = {T} MeV\", fontsize=14)\n",
    "plt.xlabel(\"L\"); plt.ylabel(\"V\"); plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "**Visualization of a and b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_arr = np.stack(a_hist)   # shape (EPOCHS, N)\n",
    "b_arr = np.stack(b_hist)   # shape (EPOCHS, N)\n",
    "epochs = np.arange(1, len(a_hist)+1)\n",
    "\n",
    "n_a = a_arr.shape[1]\n",
    "n_b = b_arr.shape[1]\n",
    "\n",
    "# palettes: darkest for index 0, lightest for highest index\n",
    "warm = plt.cm.OrRd(np.linspace(0.9, 0.45, n_a))   # a: warm colors\n",
    "cold = plt.cm.Blues(np.linspace(0.9, 0.45, n_b))  # b: cool colors\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "for i in range(n_a):\n",
    "    plt.plot(epochs, a_arr[:, i], color=warm[i], label=f\"a_{i+1}\")\n",
    "for i in range(n_b):\n",
    "    plt.plot(epochs, b_arr[:, i], color=cold[i], linestyle='--', label=f\"b_{i+1}\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"coefficient value\")\n",
    "plt.legend(ncol=2)\n",
    "plt.title(f\"T = {T} MeV, network\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ab(model):\n",
    "    a = model.a.detach().cpu().numpy()\n",
    "    b = model.b.detach().cpu().numpy()\n",
    "\n",
    "    # Plain readable print\n",
    "    print(\" T = \", T, \" MeV\")\n",
    "    print(\"a:\", \", \".join([f\"a{i} = {a[i]:.10g}\" for i in range(len(a))]))\n",
    "    print(\"b:\", \", \".join([f\"b{i} = {b[i]:.10g}\" for i in range(len(b))]))\n",
    "    print(f\"coef = {model.logcoef.exp().item():.10g}, shift = {model.shift.item():.10g}\")\n",
    "\n",
    "    # Mathematica rows\n",
    "    print(\"\\n Mathematica list (can copypaste it)\")\n",
    "    print(\"{\", f\"{T},\", \" (* Temperature in MeV *)\")\n",
    "    print(\"{\", \", \".join([f\"{x:.17g}\" for x in a]), \"},  (* a0..aN *)\")\n",
    "    print(\"{\", \", \".join([f\"{x:.17g}\" for x in b]), \"},  (* b0..bN *)\")\n",
    "    print(model.logcoef.exp().item(),\",\", model.shift.item(), \" (* coef, shift *)\",\"}\")\n",
    "\n",
    "print_ab(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
